{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9460622,"sourceType":"datasetVersion","datasetId":5751643}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 多路召回","metadata":{}},{"cell_type":"code","source":"# 升级pip（确保版本兼容）\n!pip install --upgrade pip -q\n\n# 安装faiss-cpu（Kaggle环境100%能装）\n!pip install faiss-cpu -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 强制重装protobuf到用户目录（--user是关键）\n!pip install --user --force-reinstall protobuf==3.20.3 -q\n\n# 同时确保faiss-cpu也安装到用户目录（兜底）\n!pip install --user faiss-cpu -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 强制重装protobuf 3.20.3（兼容faiss/TensorFlow，且能被Python识别）\n!pip install --force-reinstall protobuf==3.20.3 -q\n\n# 同时确保faiss-cpu已安装（兜底）\n!pip install faiss-cpu -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_recall = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/datawhalechina/fun-rec.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 进入fun-rec仓库根目录\n# %cd fun-rec\n\n# # 安装funrec库及其依赖（-e表示 editable模式，方便代码修改生效）\n# !pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 安装gdown用于下载谷歌云盘文件\n# !pip install gdown\n\n# # 下载数据（链接来自官方指南）\n# !gdown 1NAsN1Bqf88Ag2KYmGhTKDiX0Uty2dDw3\n\n# # 解压数据（假设下载的文件为.zip格式，根据实际文件名调整）\n# !unzip -q funrec_data.zip -d /content/funrec_data  # 解压到指定目录","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# try:\n#     from funrec.features.feature_column import FeatureColumn\n#     from funrec.training.trainer import train_model\n#     print(\"funrec 安装成功！\")\n# except ImportError as e:\n#     print(f\"安装失败：{e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, math, warnings, math, pickle, random\nfrom pathlib import Path\nfrom datetime import datetime\nfrom collections import defaultdict\nimport logging\nwarnings.filterwarnings('ignore')\nos.environ['OMP_NUM_THREADS'] = '1'\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nimport faiss\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import math\n# import warnings\n# import pickle\n# import random\n# from pathlib import Path\n# from datetime import datetime\n# from collections import defaultdict\n# import logging\n\n# # ========== 新增：抑制CUDA重复注册警告 + 适配protobuf ==========\n# os.environ['OMP_NUM_THREADS'] = '1'\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 屏蔽TensorFlow的CUDA警告\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'  # 强制使用python版protobuf，避免C++版本冲突\n# # ========== 警告过滤 ==========\n# warnings.filterwarnings('ignore')\n# logging.getLogger('tensorflow').setLevel(logging.ERROR)  # 屏蔽TensorFlow日志\n# logging.getLogger('faiss').setLevel(logging.ERROR)       # 屏蔽faiss日志\n\n# # ========== 初始化日志 ==========\n# logger = logging.getLogger(__name__)\n# logger.setLevel(logging.INFO)\n\n# # ========== 核心库导入（调整顺序，先导入numpy/pandas，再导入faiss/TensorFlow） ==========\n# import numpy as np\n# import pandas as pd\n# from tqdm import tqdm\n\n# # faiss导入\n# import faiss\n\n# # sklearn导入\n# from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n\n# # TensorFlow导入（最后导入，避免与faiss冲突）\n# import tensorflow as tf\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# # ========== 验证是否运行正常 ==========\n# print(\"✅ 所有库导入成功！\")\n# print(f\"faiss版本：{faiss.__version__}\")\n# print(f\"TensorFlow版本：{tf.__version__}\")\n# print(f\"protobuf版本：{__import__('protobuf').__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 将字符串路径转换为 Path 对象\ndata_path = Path('/kaggle/input/tianchinewsrec/')\nsave_path = Path('/kaggle/working')\n\n# 确保保存路径存在\nif not save_path.exists():\n    save_path.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 读取数据","metadata":{}},{"cell_type":"code","source":"# debug模式： 从训练集中划出一部分数据来调试代码\ndef get_all_click_sample(data_path, sample_nums=10000):\n    \"\"\"\n        训练集中采样一部分数据调试\n        data_path: 原数据的存储路径\n        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）\n    \"\"\"\n    all_click = pd.read_csv(data_path / 'train_click_log.csv')\n    all_user_ids = all_click.user_id.unique()\n\n    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False)\n    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n\n    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n    return all_click\n\n# 读取点击数据，这里分成线上和线下，如果是为了获取线上提交结果应该讲测试集中的点击数据合并到总的数据中\n# 如果是为了线下验证模型的有效性或者特征的有效性，可以只使用训练集\ndef get_all_click_df(data_path, offline=True):\n    if offline:\n        all_click = pd.read_csv(data_path / 'train_click_log.csv')\n    else:\n        trn_click = pd.read_csv(data_path / 'train_click_log.csv')\n        tst_click = pd.read_csv(data_path / 'testA_click_log.csv')\n\n        # all_click = trn_click.append(tst_click)\n        all_click = pd.concat([trn_click, tst_click]).reset_index(drop=True)\n\n    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n    return all_click","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 读取文章的基本属性\ndef get_item_info_df(data_path):\n    item_info_df = pd.read_csv(data_path / 'articles.csv')\n\n    # 为了方便与训练集中的click_article_id拼接，需要把article_id修改成click_article_id\n    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n\n    return item_info_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 读取文章的Embedding数据\ndef get_item_emb_dict(data_path):\n    item_emb_df = pd.read_csv(data_path / 'articles_emb.csv')\n\n    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n    # 进行归一化\n    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n\n    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n    pickle.dump(item_emb_dict, open(save_path / 'item_content_emb.pkl', 'wb'))\n\n    return item_emb_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_min_scaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 采样数据\n#all_click_df = get_all_click_sample(data_path, sample_nums=10000)  # sample_nums指定采样用户数\n# 全量训练集\nall_click_df = get_all_click_df(data_path, offline=False)\n\n# 对时间戳进行归一化,用于在关联规则的时候计算权重\nall_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_click_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"item_info_df = get_item_info_df(data_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"item_emb_dict = get_item_emb_dict(data_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 工具函数","metadata":{}},{"cell_type":"markdown","source":"### 获取用户-文章-时间函数","metadata":{}},{"cell_type":"code","source":"# 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\ndef get_user_item_time(click_df):\n\n    click_df = click_df.sort_values('click_timestamp')\n\n    def make_item_time_pair(df):\n        return list(zip(df['click_article_id'], df['click_timestamp']))\n\n    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']].apply(lambda x: make_item_time_pair(x))\\\n                                                            .reset_index().rename(columns={0: 'item_time_list'})\n    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n\n    return user_item_time_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取文章-用户-时间函数","metadata":{}},{"cell_type":"code","source":"# 根据时间获取商品被点击的用户序列  {item1: {user1: time1, user2: time2...}...}\n# 这里的时间是用户点击当前商品的时间，好像没有直接的关系。\ndef get_item_user_time_dict(click_df):\n    def make_user_time_pair(df):\n        return list(zip(df['user_id'], df['click_timestamp']))\n\n    click_df = click_df.sort_values('click_timestamp')\n    item_user_time_df = click_df.groupby('click_article_id')[['user_id', 'click_timestamp']].apply(lambda x: make_user_time_pair(x))\\\n                                                            .reset_index().rename(columns={0: 'user_time_list'})\n\n    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n    return item_user_time_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取历史和最后一次点击","metadata":{}},{"cell_type":"code","source":"# 获取当前数据的历史点击和最后一次点击\ndef get_hist_and_last_click(all_click):\n\n    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n    click_last_df = all_click.groupby('user_id').tail(1)\n\n    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n    def hist_func(user_df):\n        if len(user_df) == 1:\n            return user_df\n        else:\n            return user_df[:-1]\n\n    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n\n    return click_hist_df, click_last_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取文章属性特征","metadata":{}},{"cell_type":"code","source":"# 获取文章id对应的基本属性，保存成字典的形式，方便后面召回阶段，冷启动阶段直接使用\ndef get_item_info_dict(item_info_df):\n    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)\n\n    item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n\n    return item_type_dict, item_words_dict, item_created_time_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取用户历史点击的文章信息","metadata":{}},{"cell_type":"code","source":"def get_user_hist_item_info_dict(all_click):\n\n    # 获取user_id对应的用户历史点击文章类型的集合字典\n    user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n    user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n\n    # 获取user_id对应的用户点击文章的集合\n    user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n\n    # 获取user_id对应的用户历史点击的文章的平均字数字典\n    user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n\n    # 获取user_id对应的用户最后一次点击的文章的创建时间\n    all_click_ = all_click.sort_values('click_timestamp')\n    user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n\n    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n\n    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n                                                user_last_item_created_time['created_at_ts']))\n\n    return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取点击次数最多的Top-k个文章","metadata":{}},{"cell_type":"code","source":"# 获取近期点击最多的文章\ndef get_item_topk_click(click_df, k):\n    topk_click = click_df['click_article_id'].value_counts().index[:k]\n    return topk_click","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 定义多路召回字典","metadata":{}},{"cell_type":"code","source":"# 获取文章的属性信息，保存成字典的形式方便查询\nitem_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中\nuser_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n                           'embedding_sim_item_recall': {},\n                           'cold_start_recall': {}}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 提取最后一次点击作为召回评估，如果不需要做召回评估直接使用全量的训练集进行召回(线下验证模型)\n# 如果不是召回评估，直接使用全量数据进行召回，不用将最后一次提取出来\ntrn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 召回效果评估","metadata":{}},{"cell_type":"code","source":"# 依次评估召回的前10, 20, 30, 40, 50个文章中的击中率\ndef metrics_recall(user_recall_items_dict, trn_last_click_df, topk=5):\n    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n    user_num = len(user_recall_items_dict)\n\n    for k in range(10, topk+1, 10):\n        hit_num = 0\n        for user, item_list in user_recall_items_dict.items():\n            # 获取前k个召回的结果\n            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n            if last_click_item_dict[user] in set(tmp_recall_items):\n                hit_num += 1\n\n        hit_rate = round(hit_num * 1.0 / user_num, 5)\n        print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', user_num)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 计算相似性矩阵","metadata":{}},{"cell_type":"markdown","source":"### itemCF i2i_sim","metadata":{}},{"cell_type":"code","source":"def itemcf_sim(df, item_created_time_dict):\n    \"\"\"\n        文章与文章之间的相似性矩阵计算\n        :param df: 数据表\n        :item_created_time_dict:  文章创建时间的字典\n        return : 文章与文章的相似性矩阵\n\n        思路: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则\n    \"\"\"\n\n    user_item_time_dict = get_user_item_time(df)\n\n    # 计算物品相似度\n    i2i_sim = {}\n    item_cnt = defaultdict(int)\n    for user, item_time_list in tqdm(user_item_time_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n        # 在基于商品的协同过滤优化的时候可以考虑时间因素\n        for loc1, (i, i_click_time) in enumerate(item_time_list):\n            item_cnt[i] += 1\n            i2i_sim.setdefault(i, {})\n            for loc2, (j, j_click_time) in enumerate(item_time_list):\n                if(i == j):\n                    continue\n\n                # 考虑文章的正向顺序点击和反向顺序点击\n                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n                # 位置信息权重，其中的参数可以调节\n                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n                # 点击时间权重，其中的参数可以调节\n                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n                # 两篇文章创建时间的权重，其中的参数可以调节\n                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n                i2i_sim[i].setdefault(j, 0)\n                # 考虑多种因素的权重计算最终的文章之间的相似度\n                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n\n    i2i_sim_ = i2i_sim.copy()\n    for i, related_items in i2i_sim.items():\n        for j, wij in related_items.items():\n            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n\n    # 将得到的相似性矩阵保存到本地\n    pickle.dump(i2i_sim_, open(save_path / 'itemcf_i2i_sim.pkl', 'wb'))\n\n    return i2i_sim_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### userCF u2u_sim","metadata":{}},{"cell_type":"code","source":"def get_user_activate_degree_dict(all_click_df):\n    all_click_df_ = all_click_df.groupby('user_id')['click_article_id'].count().reset_index()\n\n    # 用户活跃度归一化\n    mm = MinMaxScaler()\n    all_click_df_['click_article_id'] = mm.fit_transform(all_click_df_[['click_article_id']])\n    user_activate_degree_dict = dict(zip(all_click_df_['user_id'], all_click_df_['click_article_id']))\n\n    return user_activate_degree_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def usercf_sim(all_click_df, user_activate_degree_dict):\n    \"\"\"\n        用户相似性矩阵计算\n        :param all_click_df: 数据表\n        :param user_activate_degree_dict: 用户活跃度的字典\n        return 用户相似性矩阵\n\n        思路: 基于用户的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则\n    \"\"\"\n    item_user_time_dict = get_item_user_time_dict(all_click_df)\n\n    u2u_sim = {}\n    user_cnt = defaultdict(int)\n    for item, user_time_list in tqdm(item_user_time_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n        for u, click_time in user_time_list:\n            user_cnt[u] += 1\n            u2u_sim.setdefault(u, {})\n            for v, click_time in user_time_list:\n                u2u_sim[u].setdefault(v, 0)\n                if u == v:\n                    continue\n                # 用户平均活跃度作为活跃度的权重，这里的式子也可以改善\n                activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])\n                u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)\n\n    u2u_sim_ = u2u_sim.copy()\n    for u, related_users in u2u_sim.items():\n        for v, wij in related_users.items():\n            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])\n\n    # 将得到的相似性矩阵保存到本地\n    pickle.dump(u2u_sim_, open(save_path / 'usercf_u2u_sim.pkl', 'wb'))\n\n    return u2u_sim_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 由于usercf计算时候太耗费内存了，这里就不直接运行了\n# 如果是采样的话，是可以运行的\nuser_activate_degree_dict = get_user_activate_degree_dict(all_click_df)\nu2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### item embedding sim","metadata":{}},{"cell_type":"code","source":"# 向量检索相似度计算\n# topk指的是每个item, faiss搜索后返回最相似的topk个item\ndef embdding_sim(click_df, item_emb_df, save_path, topk):\n    \"\"\"\n        基于内容的文章embedding相似性矩阵计算\n        :param click_df: 数据表\n        :param item_emb_df: 文章的embedding\n        :param save_path: 保存路径\n        :patam topk: 找最相似的topk篇\n        return 文章相似性矩阵\n\n        思路: 对于每一篇文章， 基于embedding的相似性返回topk个与其最相似的文章， 只不过由于文章数量太多，这里用了faiss进行加速\n    \"\"\"\n\n    # 文章索引与文章id的字典映射\n    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df['article_id']))\n\n    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n    # 向量进行单位化\n    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n\n    # 建立faiss索引\n    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])\n    item_index.add(item_emb_np)\n    # 相似度查询，给每个索引位置上的向量返回topk个item以及相似度\n    sim, idx = item_index.search(item_emb_np, topk) # 返回的是列表\n\n    # 将向量检索的结果保存成原始id的对应关系\n    item_sim_dict = defaultdict(dict)\n    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n        target_raw_id = item_idx_2_rawid_dict[target_idx]\n        # 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1\n        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n\n    # 保存i2i相似度矩阵\n    pickle.dump(item_sim_dict, open(save_path / 'emb_i2i_sim.pkl', 'wb'))\n\n    return item_sim_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: 这里需要修改, 因为usercf_sim计算太耗费内存了，暂时先采样\nitem_emb_df = pd.read_csv(data_path / 'articles_emb.csv').sample(10000, random_state=0).reset_index(drop=True)\nemb_i2i_sim = embdding_sim(all_click_df, item_emb_df, save_path, topk=10) # topk可以自行设置","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 召回","metadata":{}},{"cell_type":"markdown","source":"### YoutubeDNN","metadata":{}},{"cell_type":"markdown","source":"### itemCF recall","metadata":{}},{"cell_type":"code","source":"# 基于商品的召回i2i\ndef item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim):\n    \"\"\"\n        基于文章协同过滤的召回\n        :param user_id: 用户id\n        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\n        :param i2i_sim: 字典，文章相似性矩阵\n        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章\n        :param recall_item_num: 整数， 最后的召回文章数量\n        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n\n        return: 召回的文章列表 {item1:score1, item2: score2...}\n\n    \"\"\"\n    # 获取用户历史交互的文章\n    user_hist_items = user_item_time_dict[user_id]\n\n    item_rank = {}\n    for loc, (i, click_time) in enumerate(user_hist_items):\n        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n            if j in user_hist_items:\n                continue\n\n            # 文章创建时间差权重\n            created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n            # 相似文章和历史点击文章序列中历史文章所在的位置权重\n            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n\n            content_weight = 1.0\n            if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n                content_weight += emb_i2i_sim[i][j]\n            if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n                content_weight += emb_i2i_sim[j][i]\n\n            item_rank.setdefault(j, 0)\n            item_rank[j] += created_time_weight * loc_weight * content_weight * wij\n\n    # 不足10个，用热门商品补全\n    if len(item_rank) < recall_item_num:\n        for i, item in enumerate(item_topk_click):\n            if item in item_rank.items(): # 填充的item应该不在原来的列表中\n                continue\n            item_rank[item] = - i - 100 # 随便给个负数就行\n            if len(item_rank) == recall_item_num:\n                break\n\n    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n\n    return item_rank","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### itemCF sim召回","metadata":{}},{"cell_type":"code","source":"# 先进行itemcf召回, 为了召回评估，所以提取最后一次点击\n\nif metric_recall:\n    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\nelse:\n    trn_hist_click_df = all_click_df\n\nuser_recall_items_dict = defaultdict(dict)\nuser_item_time_dict = get_user_item_time(trn_hist_click_df)\n\ni2i_sim = pickle.load(open(save_path / 'itemcf_i2i_sim.pkl', 'rb'))\nemb_i2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl', 'rb'))\n\nsim_item_topk = 20\nrecall_item_num = 10\nitem_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n\nfor user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \\\n                                                        i2i_sim, sim_item_topk, recall_item_num, \\\n                                                        item_topk_click, item_created_time_dict, emb_i2i_sim)\n\nuser_multi_recall_dict['itemcf_sim_itemcf_recall'] = user_recall_items_dict\npickle.dump(user_multi_recall_dict['itemcf_sim_itemcf_recall'], open(save_path / 'itemcf_recall_dict.pkl', 'wb'))\n\nif metric_recall:\n    # 召回效果评估\n    metrics_recall(user_multi_recall_dict['itemcf_sim_itemcf_recall'], trn_last_click_df, topk=recall_item_num)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### embedding sim 召回","metadata":{}},{"cell_type":"code","source":"# 这里是为了召回评估，所以提取最后一次点击\nif metric_recall:\n    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\nelse:\n    trn_hist_click_df = all_click_df\n\nuser_recall_items_dict = defaultdict(dict)\nuser_item_time_dict = get_user_item_time(trn_hist_click_df)\ni2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl','rb'))\n\nsim_item_topk = 20\nrecall_item_num = 10\n\nitem_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n\nfor user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n\nuser_multi_recall_dict['embedding_sim_item_recall'] = user_recall_items_dict\npickle.dump(user_multi_recall_dict['embedding_sim_item_recall'], open(save_path / 'embedding_sim_item_recall.pkl', 'wb'))\n\nif metric_recall:\n    # 召回效果评估\n    metrics_recall(user_multi_recall_dict['embedding_sim_item_recall'], trn_last_click_df, topk=recall_item_num)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### userCF召回","metadata":{}},{"cell_type":"code","source":"# 基于用户的召回 u2u2i\ndef user_based_recommend(user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num,\n                         item_topk_click, item_created_time_dict, emb_i2i_sim):\n    \"\"\"\n        基于文章协同过滤的召回\n        :param user_id: 用户id\n        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\n        :param u2u_sim: 字典，文章相似性矩阵\n        :param sim_user_topk: 整数， 选择与当前用户最相似的前k个用户\n        :param recall_item_num: 整数， 最后的召回文章数量\n        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n        :param item_created_time_dict: 文章创建时间列表\n        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n\n        return: 召回的文章列表 {item1:score1, item2: score2...}\n    \"\"\"\n    # 历史交互\n    user_item_time_list = user_item_time_dict[user_id]    # {item1: time1, item2: time2...}\n    user_hist_items = set([i for i, t in user_item_time_list])   # 存在一个用户与某篇文章的多次交互， 这里得去重\n\n    items_rank = {}\n    for sim_u, wuv in sorted(u2u_sim[user_id].items(), key=lambda x: x[1], reverse=True)[:sim_user_topk]:\n        for i, click_time in user_item_time_dict[sim_u]:\n            if i in user_hist_items:\n                continue\n            items_rank.setdefault(i, 0)\n\n            loc_weight = 1.0\n            content_weight = 1.0\n            created_time_weight = 1.0\n\n            # 当前文章与该用户看的历史文章进行一个权重交互\n            for loc, (j, click_time) in enumerate(user_item_time_list):\n                # 点击时的相对位置权重\n                loc_weight += 0.9 ** (len(user_item_time_list) - loc)\n                # 内容相似性权重\n                if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n                    content_weight += emb_i2i_sim[i][j]\n                if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n                    content_weight += emb_i2i_sim[j][i]\n\n                # 创建时间差权重\n                created_time_weight += np.exp(0.8 * np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n\n            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv\n\n    # 热度补全\n    if len(items_rank) < recall_item_num:\n        for i, item in enumerate(item_topk_click):\n            if item in items_rank.items(): # 填充的item应该不在原来的列表中\n                continue\n            items_rank[item] = - i - 100 # 随便给个复数就行\n            if len(items_rank) == recall_item_num:\n                break\n\n    items_rank = sorted(items_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n\n    return items_rank","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### userCF sim召回","metadata":{}},{"cell_type":"code","source":"# 这里是为了召回评估，所以提取最后一次点击\n# 由于usercf中计算user之间的相似度的过程太费内存了，全量数据这里就没有跑，跑了一个采样之后的数据\nif metric_recall:\n    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\nelse:\n    trn_hist_click_df = all_click_df\n\nuser_recall_items_dict = defaultdict(dict)\nuser_item_time_dict = get_user_item_time(trn_hist_click_df)\n\nu2u_sim = pickle.load(open(save_path / 'usercf_u2u_sim.pkl', 'rb'))\n\nsim_user_topk = 20\nrecall_item_num = 10\nitem_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n\nfor user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n\npickle.dump(user_recall_items_dict, open(save_path / 'usercf_u2u2i_recall.pkl', 'wb'))\n\nif metric_recall:\n    # 召回效果评估\n    metrics_recall(user_recall_items_dict, trn_last_click_df, topk=recall_item_num)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 冷启动问题","metadata":{}},{"cell_type":"code","source":"# 先进行itemcf召回，这里不需要做召回评估，这里只是一种策略\ntrn_hist_click_df = all_click_df\n\nuser_recall_items_dict = defaultdict(dict)\nuser_item_time_dict = get_user_item_time(trn_hist_click_df)\ni2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl','rb'))\n\nsim_item_topk = 150\nrecall_item_num = 100 # 稍微召回多一点文章，便于后续的规则筛选\n\nitem_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\nfor user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n                                                        recall_item_num, item_topk_click,item_created_time_dict, emb_i2i_sim)\npickle.dump(user_recall_items_dict, open(save_path / 'cold_start_items_raw_dict.pkl', 'wb'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 基于规则进行文章过滤\n# 保留文章主题与用户历史浏览主题相似的文章\n# 保留文章字数与用户历史浏览文章字数相差不大的文章\n# 保留最后一次点击当天的文章\n# 按照相似度返回最终的结果\n\ndef get_click_article_ids_set(all_click_df):\n    return set(all_click_df.click_article_id.values)\n\ndef cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n                     user_last_item_created_time_dict, item_type_dict, item_words_dict,\n                     item_created_time_dict, click_article_ids_set, recall_item_num):\n    \"\"\"\n        冷启动的情况下召回一些文章\n        :param user_recall_items_dict: 基于内容embedding相似性召回来的很多文章， 字典， {user1: [item1, item2, ..], }\n        :param user_hist_item_typs_dict: 字典， 用户点击的文章的主题映射\n        :param user_hist_item_words_dict: 字典， 用户点击的历史文章的字数映射\n        :param user_last_item_created_time_idct: 字典，用户点击的历史文章创建时间映射\n        :param item_tpye_idct: 字典，文章主题映射\n        :param item_words_dict: 字典，文章字数映射\n        :param item_created_time_dict: 字典， 文章创建时间映射\n        :param click_article_ids_set: 集合，用户点击过得文章, 也就是日志里面出现过的文章\n        :param recall_item_num: 召回文章的数量， 这个指的是没有出现在日志里面的文章数量\n    \"\"\"\n\n    cold_start_user_items_dict = {}\n    for user, item_list in tqdm(user_recall_items_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n        cold_start_user_items_dict.setdefault(user, [])\n        for item, score in item_list:\n            # 获取历史文章信息\n            hist_item_type_set = user_hist_item_typs_dict[user]\n            hist_mean_words = user_hist_item_words_dict[user]\n            hist_last_item_created_time = user_last_item_created_time_dict[user]\n            hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)\n\n            # 获取当前召回文章的信息\n            curr_item_type = item_type_dict[item]\n            curr_item_words = item_words_dict[item]\n            curr_item_created_time = item_created_time_dict[item]\n            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)\n\n            # 首先，文章不能出现在用户的历史点击中， 然后根据文章主题，文章单词数，文章创建时间进行筛选\n            if curr_item_type not in hist_item_type_set or \\\n                item in click_article_ids_set or \\\n                abs(curr_item_words - hist_mean_words) > 200 or \\\n                abs((curr_item_created_time - hist_last_item_created_time).days) > 90:\n                continue\n\n            cold_start_user_items_dict[user].append((item, score))      # {user1: [(item1, score1), (item2, score2)..]...}\n\n    # 需要控制一下冷启动召回的数量\n    cold_start_user_items_dict = {k: sorted(v, key=lambda x:x[1], reverse=True)[:recall_item_num] \\\n                                  for k, v in cold_start_user_items_dict.items()}\n\n    pickle.dump(cold_start_user_items_dict, open(save_path / 'cold_start_user_items_dict.pkl', 'wb'))\n\n    return cold_start_user_items_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_click_df_ = all_click_df.copy()\nall_click_df_ = all_click_df_.merge(item_info_df, how='left', on='click_article_id')\nuser_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)\nclick_article_ids_set = get_click_article_ids_set(all_click_df)\n# 需要注意的是\n# 这里使用了很多规则来筛选冷启动的文章，所以前面再召回的阶段就应该尽可能的多召回一些文章，否则很容易被删掉\ncold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \\\n                                              item_created_time_dict, click_article_ids_set, recall_item_num)\n\nuser_multi_recall_dict['cold_start_recall'] = cold_start_user_items_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 多路召回合并","metadata":{}},{"cell_type":"code","source":"def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25):\n    final_recall_items_dict = {}\n\n    # 对每一种召回结果按照用户进行归一化，方便后面多种召回结果，相同用户的物品之间权重相加\n    def norm_user_recall_items_sim(sorted_item_list):\n        # 如果冷启动中没有文章或者只有一篇文章，直接返回，出现这种情况的原因可能是冷启动召回的文章数量太少了，\n        # 基于规则筛选之后就没有文章了, 这里还可以做一些其他的策略性的筛选\n        if len(sorted_item_list) < 2:\n            return sorted_item_list\n\n        min_sim = sorted_item_list[-1][1]\n        max_sim = sorted_item_list[0][1]\n\n        norm_sorted_item_list = []\n        for item, score in sorted_item_list:\n            if max_sim > 0:\n                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0\n            else:\n                norm_score = 0.0\n            norm_sorted_item_list.append((item, norm_score))\n\n        return norm_sorted_item_list\n\n    print('多路召回合并...')\n    for method, user_recall_items in tqdm(user_multi_recall_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n        print(method + '...')\n        # 在计算最终召回结果的时候，也可以为每一种召回结果设置一个权重\n        if weight_dict == None:\n            recall_method_weight = 1\n        else:\n            recall_method_weight = weight_dict[method]\n\n        for user_id, sorted_item_list in user_recall_items.items(): # 进行归一化\n            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)\n\n        for user_id, sorted_item_list in user_recall_items.items():\n            # print('user_id')\n            final_recall_items_dict.setdefault(user_id, {})\n            for item, score in sorted_item_list:\n                final_recall_items_dict[user_id].setdefault(item, 0)\n                final_recall_items_dict[user_id][item] += recall_method_weight * score\n\n    final_recall_items_dict_rank = {}\n    # 多路召回时也可以控制最终的召回数量\n    for user, recall_item_dict in final_recall_items_dict.items():\n        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]\n\n    # 将多路召回后的最终结果字典保存到本地\n    pickle.dump(final_recall_items_dict, open(os.path.join(save_path, 'final_recall_items_dict.pkl'),'wb'))\n\n    return final_recall_items_dict_rank","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 这里直接对多路召回的权重给了一个相同的值，其实可以根据前面召回的情况来调整参数的值\n# weight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n#                'embedding_sim_item_recall': 1.0,\n#                'youtubednn_recall': 1.0,\n#                'youtubednn_usercf_recall': 1.0,\n#                'cold_start_recall': 1.0}\nweight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n               'embedding_sim_item_recall': 1.0,\n               'cold_start_recall': 1.0}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 最终合并之后每个用户召回150个商品进行排序\nfinal_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, weight_dict, topk=150)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_recall_items_dict_rank","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 特征工程","metadata":{}},{"cell_type":"code","source":"import gc, os\nimport logging\nimport pickle\nimport time\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom gensim.models import Word2Vec\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 节省内存的一个函数\n# 减少内存\ndef reduce_mem(df):\n    starttime = time.time()\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if pd.isnull(c_min) or pd.isnull(c_max):\n                continue\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n                                                                                                           100*(start_mem-end_mem)/start_mem,\n                                                                                                           (time.time()-starttime)/60))\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 将字符串路径转换为 Path 对象\ndata_path = Path('/kaggle/input/tianchinewsrec/')\nsave_path = Path('/kaggle/working')\n\n# 确保保存路径存在\nif not save_path.exists():\n    save_path.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 数据读取","metadata":{}},{"cell_type":"markdown","source":"### 训练和验证集的划分","metadata":{}},{"cell_type":"code","source":"# all_click_df指的是训练集\n# sample_user_nums 采样作为验证集的用户数量\ndef trn_val_split(all_click_df, sample_user_nums):\n    all_click = all_click_df\n    all_user_ids = all_click.user_id.unique()\n\n    # replace=True表示可以重复抽样，反之不可以\n    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=False)\n\n    click_val = all_click[all_click['user_id'].isin(sample_user_ids)]\n    click_trn = all_click[~all_click['user_id'].isin(sample_user_ids)]\n\n    # 将验证集中的最后一次点击给抽取出来作为答案\n    click_val = click_val.sort_values(['user_id', 'click_timestamp'])\n    val_ans = click_val.groupby('user_id').tail(1)\n\n    click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)\n\n    # 去除val_ans中某些用户只有一个点击数据的情况，如果该用户只有一个点击数据，又被分到ans中，\n    # 那么训练集中就没有这个用户的点击数据，出现用户冷启动问题，给自己模型验证带来麻烦\n    val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] # 保证答案中出现的用户再验证集中还有\n    click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]\n\n    return click_trn, click_val, val_ans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 获取历史点击和最后一次点击","metadata":{}},{"cell_type":"code","source":"# 获取当前数据的历史点击和最后一次点击\ndef get_hist_and_last_click(all_click):\n    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n    click_last_df = all_click.groupby('user_id').tail(1)\n\n    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n    def hist_func(user_df):\n        if len(user_df) == 1:\n            return user_df\n        else:\n            return user_df[:-1]\n\n    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n\n    return click_hist_df, click_last_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 读取训练、验证及测试集","metadata":{}},{"cell_type":"code","source":"def get_trn_val_tst_data(data_path, offline=True):\n    if offline:\n        click_trn_data = pd.read_csv(data_path / 'train_click_log.csv')  # 训练集用户点击日志\n        click_trn_data = reduce_mem(click_trn_data)\n        click_trn, click_val, val_ans = trn_val_split(all_click_df, sample_user_nums)\n    else:\n        click_trn = pd.read_csv(data_path / 'train_click_log.csv')\n        click_trn = reduce_mem(click_trn)\n        click_val = None\n        val_ans = None\n\n    click_tst = pd.read_csv(data_path / 'testA_click_log.csv')\n\n    return click_trn, click_val, click_tst, val_ans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 读取召回列表","metadata":{}},{"cell_type":"code","source":"# 返回多路召回列表或者单路召回\ndef get_recall_list(save_path, single_recall_model=None, multi_recall=False):\n    if multi_recall:\n        return pickle.load(open(save_path / 'final_recall_items_dict.pkl', 'rb'))\n\n    if single_recall_model == 'i2i_itemcf':\n        return pickle.load(open(save_path / 'itemcf_recall_dict.pkl', 'rb'))\n    elif single_recall_model == 'i2i_emb_itemcf':\n        return pickle.load(open(save_path / 'itemcf_emb_dict.pkl', 'rb'))\n    elif single_recall_model == 'user_cf':\n        return pickle.load(open(save_path / 'youtubednn_usercf_dict.pkl', 'rb'))\n    elif single_recall_model == 'youtubednn':\n        return pickle.load(open(save_path / 'youtube_u2i_dict.pkl', 'rb'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 读取各种Embedding","metadata":{}},{"cell_type":"markdown","source":"#### Word2Vec训练及gensim的使用","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\ndef trian_item_word2vec(click_df, embed_size=64, save_name='item_w2v_emb.pkl', split_char=' '):\n    click_df = click_df.sort_values('click_timestamp')\n    # 只有转换成字符串才可以进行训练\n    click_df['click_article_id'] = click_df['click_article_id'].astype(str)\n    # 转换成句子的形式\n    docs = click_df.groupby(['user_id'])['click_article_id'].apply(lambda x: list(x)).reset_index()\n    docs = docs['click_article_id'].values.tolist()\n\n    # 为了方便查看训练的进度，这里设定一个log信息\n    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n\n    # 这里的参数对训练得到的向量影响也很大,默认负采样为5\n    model = Word2Vec(docs, vector_size=16, sg=1, window=5, seed=2020, workers=24, min_count=1, epochs=1)\n\n    # 保存成字典的形式\n    item_w2v_emb_dict = {k: model.wv[k] for k in click_df['click_article_id']}\n    pickle.dump(item_w2v_emb_dict, open(save_path / 'item_w2v_emb.pkl', 'wb'))\n\n    return item_w2v_emb_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 可以通过字典查询对应的item的Embedding\ndef get_embedding(save_path, all_click_df):\n    if os.path.exists(save_path / 'item_content_emb.pkl'):\n        item_content_emb_dict = pickle.load(open(save_path / 'item_content_emb.pkl', 'rb'))\n    else:\n        print('item_content_emb.pkl 文件不存在...')\n\n    # w2v Embedding是需要提前训练好的\n    if os.path.exists(save_path / 'item_w2v_emb.pkl'):\n        item_w2v_emb_dict = pickle.load(open(save_path / 'item_w2v_emb.pkl', 'rb'))\n    else:\n        item_w2v_emb_dict = trian_item_word2vec(all_click_df)\n\n    # if os.path.exists(save_path / 'item_youtube_emb.pkl'):\n    #     item_youtube_emb_dict = pickle.load(open(save_path / 'item_youtube_emb.pkl', 'rb'))\n    # else:\n    #     print('item_youtube_emb.pkl 文件不存在...')\n\n    # if os.path.exists(save_path / 'user_youtube_emb.pkl'):\n    #     user_youtube_emb_dict = pickle.load(open(save_path / 'user_youtube_emb.pkl', 'rb'))\n    # else:\n    #     print('user_youtube_emb.pkl 文件不存在...')\n\n    return item_content_emb_dict, item_w2v_emb_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 读取文章信息","metadata":{}},{"cell_type":"code","source":"def get_article_info_df():\n    article_info_df = pd.read_csv(data_path / 'articles.csv')\n    article_info_df = reduce_mem(article_info_df)\n\n    return article_info_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 读取数据","metadata":{}},{"cell_type":"code","source":"# 这里offline的online的区别就是验证集是否为空\nclick_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)\n\nif click_val is not None:\n    click_val_hist, click_val_last = click_val, val_ans\nelse:\n    click_val_hist, click_val_last = None, None\n\nclick_tst_hist = click_tst","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 对训练数据做负采样","metadata":{}},{"cell_type":"code","source":"# 将召回列表转换成df的形式\ndef recall_dict_2_df(recall_list_dict):\n    df_row_list = [] # [user, item, score]\n    for user, recall_list in tqdm(recall_list_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n        for item, score in recall_list:\n            df_row_list.append([user, item, score])\n\n    col_names = ['user_id', 'sim_item', 'score']\n    recall_list_df = pd.DataFrame(df_row_list, columns=col_names)\n\n    return recall_list_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 负采样函数，这里可以控制负采样时的比例, 这里给了一个默认的值\ndef neg_sample_recall_data(recall_items_df, sample_rate=0.001):\n    pos_data = recall_items_df[recall_items_df['label'] == 1]\n    neg_data = recall_items_df[recall_items_df['label'] == 0]\n\n    print('pos_data_num:', len(pos_data), 'neg_data_num:', len(neg_data), 'pos/neg:', len(pos_data)/len(neg_data))\n\n    # 分组采样函数\n    def neg_sample_func(group_df):\n        neg_num = len(group_df)\n        sample_num = max(int(neg_num * sample_rate), 1) # 保证最少有一个\n        sample_num = min(sample_num, 5) # 保证最多不超过5个，这里可以根据实际情况进行选择\n        return group_df.sample(n=sample_num, replace=True)\n\n    # 对用户进行负采样，保证所有用户都在采样后的数据中\n    neg_data_user_sample = neg_data.groupby('user_id', group_keys=False).apply(neg_sample_func)\n    # 对文章进行负采样，保证所有文章都在采样后的数据中\n    neg_data_item_sample = neg_data.groupby('sim_item', group_keys=False).apply(neg_sample_func)\n\n    # 将上述两种情况下的采样数据合并\n    neg_data_new = pd.concat([neg_data_user_sample, neg_data_item_sample]).reset_index(drop=True)\n    # 由于上述两个操作是分开的，可能将两个相同的数据给重复选择了，所以需要对合并后的数据进行去重\n    neg_data_new = neg_data_new.sort_values(['user_id', 'score']).drop_duplicates(['user_id', 'sim_item'], keep='last')\n\n    # 将正样本数据合并\n    data_new = pd.concat([pos_data, neg_data_new], ignore_index=True)\n\n    return data_new","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 召回数据打标签\ndef get_rank_label_df(recall_list_df, label_df, is_test=False):\n    # 测试集是没有标签了，为了后面代码同一一些，这里直接给一个负数替代\n    if is_test:\n        recall_list_df['label'] = -1\n        return recall_list_df\n\n    label_df = label_df.rename(columns={'click_article_id': 'sim_item'})\n    recall_list_df_ = recall_list_df.merge(label_df[['user_id', 'sim_item', 'click_timestamp']], \\\n                                               how='left', on=['user_id', 'sim_item'])\n    recall_list_df_['label'] = recall_list_df_['click_timestamp'].apply(lambda x: 0.0 if np.isnan(x) else 1.0)\n    del recall_list_df_['click_timestamp']\n\n    return recall_list_df_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_user_recall_item_label_df(click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df):\n    # 获取训练数据的召回列表\n    trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n    # 训练数据打标签\n    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n    # 训练数据负采样\n    trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n\n    if click_val is not None:\n        val_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_val_hist['user_id'].unique())]\n        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=False)\n        val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)\n    else:\n        val_user_item_label_df = None\n\n    # 测试数据不需要进行负采样，直接对所有的召回商品进行打-1标签\n    tst_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_tst_hist['user_id'].unique())]\n    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, None, is_test=True)\n\n    return trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 读取召回列表\nrecall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf') # 这里只选择了单路召回的结果，也可以选择多路召回结果\n# 将召回数据转换成df\nrecall_list_df = recall_dict_2_df(recall_list_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 给训练验证数据打标签，并负采样（这一部分时间比较久）\ntrn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist,\n                                                                                                       click_val_hist,\n                                                                                                       click_tst_hist,\n                                                                                                       click_trn_last,\n                                                                                                       click_val_last,\n                                                                                                       recall_list_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trn_user_item_label_df.label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 将召回数据转化成字典","metadata":{}},{"cell_type":"code","source":"# 将最终的召回的df数据转换成字典的形式做排序特征\ndef make_tuple_func(group_df):\n    row_data = []\n    for name, row_df in group_df.iterrows():\n        row_data.append((row_df['sim_item'], row_df['score'], row_df['label']))\n\n    return row_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trn_user_item_label_tuples_dict = trn_user_item_label_df.groupby('user_id').apply(make_tuple_func).to_dict()\n\nif val_user_item_label_df is not None:\n    val_user_item_label_tuples_dict = val_user_item_label_df.groupby('user_id').apply(make_tuple_func).to_dict()\nelse:\n    val_user_item_label_tuples_dict = None\n\ntst_user_item_label_tuples_dict = tst_user_item_label_df.groupby('user_id').apply(make_tuple_func).to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 用户历史行为相关特征","metadata":{}},{"cell_type":"code","source":"# 下面基于data做历史相关的特征\ndef create_feature(users_id, recall_list, click_hist_df,  articles_info, articles_emb, user_emb=None, N=1):\n    \"\"\"\n    基于用户的历史行为做相关特征\n    :param users_id: 用户id\n    :param recall_list: 对于每个用户召回的候选文章列表\n    :param click_hist_df: 用户的历史点击信息\n    :param articles_info: 文章信息\n    :param articles_emb: 文章的embedding向量, 这个可以用item_content_emb, item_w2v_emb, item_youtube_emb\n    :param user_emb: 用户的embedding向量， 这个是user_youtube_emb, 如果没有也可以不用， 但要注意如果要用的话， articles_emb就要用item_youtube_emb的形式， 这样维度才一样\n    :param N: 最近的N次点击  由于testA日志里面很多用户只存在一次历史点击， 所以为了不产生空值，默认是1\n    \"\"\"\n\n    # 建立一个二维列表保存结果， 后面要转成DataFrame\n    all_user_feas = []\n    i = 0\n    for user_id in tqdm(users_id, disable=not logger.isEnabledFor(logging.DEBUG)):\n        # 该用户的最后N次点击\n        hist_user_items = click_hist_df[click_hist_df['user_id']==user_id]['click_article_id'][-N:]\n\n        # 遍历该用户的召回列表\n        for rank, (article_id, score, label) in enumerate(recall_list[user_id]):\n            # 该文章建立时间, 字数\n            a_create_time = articles_info[articles_info['article_id']==article_id]['created_at_ts'].values[0]\n            a_words_count = articles_info[articles_info['article_id']==article_id]['words_count'].values[0]\n            single_user_fea = [user_id, article_id]\n            # 计算与最后点击的商品的相似度的和， 最大值和最小值， 均值\n            sim_fea = []\n            time_fea = []\n            word_fea = []\n            # 遍历用户的最后N次点击文章\n            for hist_item in hist_user_items:\n                b_create_time = articles_info[articles_info['article_id']==hist_item]['created_at_ts'].values[0]\n                b_words_count = articles_info[articles_info['article_id']==hist_item]['words_count'].values[0]\n\n                sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))\n                time_fea.append(abs(a_create_time-b_create_time))\n                word_fea.append(abs(a_words_count-b_words_count))\n\n            single_user_fea.extend(sim_fea)      # 相似性特征\n            single_user_fea.extend(time_fea)    # 时间差特征\n            single_user_fea.extend(word_fea)    # 字数差特征\n            single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea)])  # 相似性的统计特征\n\n            if user_emb:  # 如果用户向量有的话， 这里计算该召回文章与用户的相似性特征\n                single_user_fea.append(np.dot(user_emb[user_id], articles_emb[article_id]))\n\n            single_user_fea.extend([score, rank, label])\n            # 加入到总的表中\n            all_user_feas.append(single_user_fea)\n\n    # 定义列名\n    id_cols = ['user_id', 'click_article_id']\n    sim_cols = ['sim' + str(i) for i in range(N)]\n    time_cols = ['time_diff' + str(i) for i in range(N)]\n    word_cols = ['word_diff' + str(i) for i in range(N)]\n    sat_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean']\n    user_item_sim_cols = ['user_item_sim'] if user_emb else []\n    user_score_rank_label = ['score', 'rank', 'label']\n    cols = id_cols + sim_cols + time_cols + word_cols + sat_cols + user_item_sim_cols + user_score_rank_label\n\n    # 转成DataFrame\n    df = pd.DataFrame( all_user_feas, columns=cols)\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"article_info_df = get_article_info_df()\n# all_click = click_trn.append(click_tst)\nall_click = pd.concat([click_trn, click_tst]).reset_index(drop=True)\nitem_content_emb_dict, item_w2v_emb_dict = get_embedding(save_path, all_click)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 获取训练验证及测试数据中召回列文章相关特征\ntrn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n                                            click_trn_hist, article_info_df, item_content_emb_dict)\n\nif val_user_item_label_tuples_dict is not None:\n    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \\\n                                                click_val_hist, article_info_df, item_content_emb_dict)\nelse:\n    val_user_item_feats_df = None\n\ntst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \\\n                                            click_tst_hist, article_info_df, item_content_emb_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if tst_user_item_feats_df.empty:\n    tst_user_item_feats_df = pd.DataFrame({\n        # 原有核心列（保留）\n        'user_id': [1],\n        'click_article_id': [300],\n        'pred_score': [0.5],  # 临时值，会被模型预测覆盖\n        'user_age': [25],     # 非lgb_cols但合并user_info需要的列\n        'article_id': [300],  # 非lgb_cols但合并articles需要的列\n        \n        # ===== lgb_cols 全量特征（按列表顺序填充模拟值）=====\n        'sim0': [0.85],                # 与用户历史第1篇文章的相似度（0-1）\n        'time_diff0': [3600],          # 与历史第1篇文章的时间差（秒，示例：1小时）\n        'word_diff0': [0.2],           # 与历史第1篇文章的词向量差异（0-1）\n        'sim_max': [0.85],             # 历史点击与当前文章的最大相似度\n        'sim_min': [0.78],             # 历史点击与当前文章的最小相似度\n        'sim_sum': [1.63],             # 历史点击与当前文章的相似度总和\n        'sim_mean': [0.815],           # 历史点击与当前文章的相似度均值\n        'score': [0.92],               # 召回阶段的初始分数（0-1）\n        'click_size': [5],             # 用户历史点击文章总数（正整数）\n        'time_diff_mean': [5400],      # 历史点击时间差均值（秒，示例：1.5小时）\n        'active_level': [2],           # 用户活跃等级（1-5，数值越大越活跃）\n        'click_environment': [1],      # 点击环境（1:APP 2:H5 3:PC）\n        'click_deviceGroup': [3],      # 设备组（1:高端 2:中端 3:低端）\n        'click_os': [2],               # 操作系统（1:iOS 2:Android 3:其他）\n        'click_country': [1],          # 国家编码（1:中国）\n        'click_region': [101],         # 地区编码（101:北京 102:上海 103:广州）\n        'click_referrer_type': [4],    # 来源类型（1:推荐 2:搜索 3:分享 4:首页 5:其他）\n        'user_time_hob1': [18],        # 用户核心活跃时段1（小时，示例：18点）\n        'user_time_hob2': [21],        # 用户核心活跃时段2（小时，示例：21点）\n        'words_hbo': [0.75],           # 用户词向量偏好匹配度（0-1）\n        'category_id': [10],           # 文章类别ID（与原有列一致）\n        'created_at_ts': ['2023-10-01 12:00:00'],  # 文章发布时间戳（字符串格式）\n        'words_count': [1200]          # 文章字数（正整数）\n    })\n\n# 可选：验证所有lgb_cols特征是否都已包含\nlgb_cols = [\n    'sim0', 'time_diff0', 'word_diff0', 'sim_max', 'sim_min', 'sim_sum',\n    'sim_mean', 'score', 'click_size', 'time_diff_mean', 'active_level',\n    'click_environment', 'click_deviceGroup', 'click_os', 'click_country',\n    'click_region', 'click_referrer_type', 'user_time_hob1', 'user_time_hob2',\n    'words_hbo', 'category_id', 'created_at_ts', 'words_count'\n]\nmissing_cols = [col for col in lgb_cols if col not in tst_user_item_feats_df.columns]\nif missing_cols:\n    print(f\"⚠️  缺失lgb_cols特征：{missing_cols}\")\nelse:\n    print(\"✅ 所有lgb_cols特征已完整包含，可执行模型预测\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 保存一份省的每次都要重新跑，每次跑的时间都比较长\ntrn_user_item_feats_df.to_csv(save_path / 'trn_user_item_feats_df.csv', index=False)\n\nif val_user_item_feats_df is not None:\n    val_user_item_feats_df.to_csv(save_path / 'val_user_item_feats_df.csv', index=False)\n\ntst_user_item_feats_df.to_csv(save_path / 'tst_user_item_feats_df.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 用户和文章特征","metadata":{}},{"cell_type":"markdown","source":"### 用户相关特征","metadata":{}},{"cell_type":"code","source":"click_tst.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 读取文章特征\narticles =  pd.read_csv(data_path / 'articles.csv')\narticles = reduce_mem(articles)\n\n# 日志数据，就是前面的所有数据\nif click_val is not None:\n    # all_data = click_trn.append(click_val)\n    all_data = pd.concat([click_trn, click_val]).reset_index(drop=True)\nall_data = pd.concat([click_trn, click_tst]).reset_index(drop=True)\nall_data = reduce_mem(all_data)\n\n# 拼上文章信息\nall_data = all_data.merge(articles, left_on='click_article_id', right_on='article_id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 分析一下点击时间和点击文章的次数，区分用户活跃度","metadata":{}},{"cell_type":"code","source":"def active_level(all_data, cols):\n    \"\"\"\n    制作区分用户活跃度的特征\n    :param all_data: 数据集\n    :param cols: 用到的特征列\n    \"\"\"\n    data = all_data[cols]\n    data.sort_values(['user_id', 'click_timestamp'], inplace=True)\n    user_act = pd.DataFrame(data.groupby('user_id', as_index=False)[['click_article_id', 'click_timestamp']].\\\n                            agg({'click_article_id':np.size, 'click_timestamp': {list}}).values, columns=['user_id', 'click_size', 'click_timestamp'])\n\n    # 计算时间间隔的均值\n    def time_diff_mean(l):\n        if len(l) == 1:\n            return 1\n        else:\n            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n\n    user_act['time_diff_mean'] = user_act['click_timestamp'].apply(lambda x: time_diff_mean(x))\n\n    # 点击次数取倒数\n    user_act['click_size'] = 1 / user_act['click_size']\n\n    # 两者归一化\n    user_act['click_size'] = (user_act['click_size'] - user_act['click_size'].min()) / (user_act['click_size'].max() - user_act['click_size'].min())\n    user_act['time_diff_mean'] = (user_act['time_diff_mean'] - user_act['time_diff_mean'].min()) / (user_act['time_diff_mean'].max() - user_act['time_diff_mean'].min())\n    user_act['active_level'] = user_act['click_size'] + user_act['time_diff_mean']\n\n    user_act['user_id'] = user_act['user_id'].astype('int')\n    del user_act['click_timestamp']\n\n    return user_act","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_act_fea = active_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_act_fea.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 分析一下点击时间和被点击文章的次数， 衡量文章热度特征","metadata":{}},{"cell_type":"code","source":"def hot_level(all_data, cols):\n    \"\"\"\n    制作衡量文章热度的特征\n    :param all_data: 数据集\n    :param cols: 用到的特征列\n    \"\"\"\n    data = all_data[cols]\n    data.sort_values(['click_article_id', 'click_timestamp'], inplace=True)\n    article_hot = pd.DataFrame(data.groupby('click_article_id', as_index=False)[['user_id', 'click_timestamp']].\\\n                               agg({'user_id':np.size, 'click_timestamp': {list}}).values, columns=['click_article_id', 'user_num', 'click_timestamp'])\n\n    # 计算被点击时间间隔的均值\n    def time_diff_mean(l):\n        if len(l) == 1:\n            return 1\n        else:\n            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n\n    article_hot['time_diff_mean'] = article_hot['click_timestamp'].apply(lambda x: time_diff_mean(x))\n\n    # 点击次数取倒数\n    article_hot['user_num'] = 1 / article_hot['user_num']\n\n    # 两者归一化\n    article_hot['user_num'] = (article_hot['user_num'] - article_hot['user_num'].min()) / (article_hot['user_num'].max() - article_hot['user_num'].min())\n    article_hot['time_diff_mean'] = (article_hot['time_diff_mean'] - article_hot['time_diff_mean'].min()) / (article_hot['time_diff_mean'].max() - article_hot['time_diff_mean'].min())\n    article_hot['hot_level'] = article_hot['user_num'] + article_hot['time_diff_mean']\n\n    article_hot['click_article_id'] = article_hot['click_article_id'].astype('int')\n\n    del article_hot['click_timestamp']\n\n    return article_hot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"article_hot_fea = hot_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"article_hot_fea.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 用户的设备习惯","metadata":{}},{"cell_type":"code","source":"def device_fea(all_data, cols):\n    \"\"\"\n    制作用户的设备特征\n    :param all_data: 数据集\n    :param cols: 用到的特征列\n    \"\"\"\n    user_device_info = all_data[cols]\n\n    # 用众数来表示每个用户的设备信息\n    user_device_info = user_device_info.groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()\n\n    return user_device_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 设备特征(这里时间会比较长)\ndevice_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\nuser_device_info = device_fea(all_data, device_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_device_info.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 用户的时间习惯","metadata":{}},{"cell_type":"code","source":"def user_time_hob_fea(all_data, cols):\n    \"\"\"\n    制作用户的时间习惯特征\n    :param all_data: 数据集\n    :param cols: 用到的特征列\n    \"\"\"\n    user_time_hob_info = all_data[cols]\n\n    # 先把时间戳进行归一化\n    mm = MinMaxScaler()\n    user_time_hob_info['click_timestamp'] = mm.fit_transform(user_time_hob_info[['click_timestamp']])\n    user_time_hob_info['created_at_ts'] = mm.fit_transform(user_time_hob_info[['created_at_ts']])\n\n    user_time_hob_info = user_time_hob_info.groupby('user_id').agg('mean').reset_index()\n\n    user_time_hob_info.rename(columns={'click_timestamp': 'user_time_hob1', 'created_at_ts': 'user_time_hob2'}, inplace=True)\n    return user_time_hob_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_time_hob_cols = ['user_id', 'click_timestamp', 'created_at_ts']\nuser_time_hob_info = user_time_hob_fea(all_data, user_time_hob_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 用户的主题爱好","metadata":{}},{"cell_type":"code","source":"def user_cat_hob_fea(all_data, cols):\n    \"\"\"\n    用户的主题爱好\n    :param all_data: 数据集\n    :param cols: 用到的特征列\n    \"\"\"\n    user_category_hob_info = all_data[cols]\n    user_category_hob_info = user_category_hob_info.groupby('user_id').agg({list}).reset_index()\n\n    user_cat_hob_info = pd.DataFrame()\n    user_cat_hob_info['user_id'] = user_category_hob_info['user_id']\n    user_cat_hob_info['cate_list'] = user_category_hob_info['category_id']\n\n    return user_cat_hob_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_category_hob_cols = ['user_id', 'category_id']\nuser_cat_hob_info = user_cat_hob_fea(all_data, user_category_hob_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 用户的字数偏好特征","metadata":{}},{"cell_type":"code","source":"user_wcou_info = all_data.groupby('user_id')['words_count'].agg('mean').reset_index()\nuser_wcou_info.rename(columns={'words_count': 'words_hbo'}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 用户的信息特征合并保存","metadata":{}},{"cell_type":"code","source":"# 所有表进行合并\nuser_info = pd.merge(user_act_fea, user_device_info, on='user_id')\nuser_info = user_info.merge(user_time_hob_info, on='user_id')\nuser_info = user_info.merge(user_cat_hob_info, on='user_id')\nuser_info = user_info.merge(user_wcou_info, on='user_id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 这样用户特征以后就可以直接读取了\nuser_info.to_csv(save_path / 'user_info.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 用户特征直接读入","metadata":{}},{"cell_type":"code","source":"# 把用户信息直接读入进来\nuser_info = pd.read_csv(save_path / 'user_info.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.exists(save_path / 'trn_user_item_feats_df.csv'):\n    trn_user_item_feats_df = pd.read_csv(save_path / 'trn_user_item_feats_df.csv')\n\nif os.path.exists(save_path / 'tst_user_item_feats_df.csv'):\n    tst_user_item_feats_df = pd.read_csv(save_path / 'tst_user_item_feats_df.csv')\n\nif os.path.exists(save_path / 'val_user_item_feats_df.csv'):\n    val_user_item_feats_df = pd.read_csv(save_path / 'val_user_item_feats_df.csv')\nelse:\n    val_user_item_feats_df = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 拼上用户特征\n# 下面是线下验证的\ntrn_user_item_feats_df = trn_user_item_feats_df.merge(user_info, on='user_id', how='left')\n\nif val_user_item_feats_df is not None:\n    val_user_item_feats_df = val_user_item_feats_df.merge(user_info, on='user_id', how='left')\nelse:\n    val_user_item_feats_df = None\n\ntst_user_item_feats_df = tst_user_item_feats_df.merge(user_info, on='user_id',how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trn_user_item_feats_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 文章的特征直接读入","metadata":{}},{"cell_type":"code","source":"articles =  pd.read_csv(data_path / 'articles.csv')\narticles = reduce_mem(articles)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 拼上文章特征\ntrn_user_item_feats_df = trn_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n\nif val_user_item_feats_df is not None:\n    val_user_item_feats_df = val_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\nelse:\n    val_user_item_feats_df = None\n\ntst_user_item_feats_df = tst_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 召回文章的主题是否在用户的爱好里面","metadata":{}},{"cell_type":"code","source":"# trn_user_item_feats_df['is_cat_hab'] = trn_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)\n# if val_user_item_feats_df is not None:\n#     val_user_item_feats_df['is_cat_hab'] = val_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)\n# else:\n#     val_user_item_feats_df = None\n# #TODO: 这里因为是sample数据原因，tst_user_item_feats_df 大小为0，当使用全量数据时，需要删除这行\n# if tst_user_item_feats_df.shape[0] > 0:\n#     tst_user_item_feats_df['is_cat_hab'] = tst_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 线下验证\n#del trn_user_item_feats_df['cate_list']\n\nif val_user_item_feats_df is not None:\n    del val_user_item_feats_df['cate_list']\nelse:\n    val_user_item_feats_df = None\n\n#del tst_user_item_feats_df['cate_list']\n\n#del trn_user_item_feats_df['article_id']\n\nif val_user_item_feats_df is not None:\n    del val_user_item_feats_df['article_id']\nelse:\n    val_user_item_feats_df = None\n\n#del tst_user_item_feats_df['article_id']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 保存特征","metadata":{}},{"cell_type":"code","source":"# 训练验证特征\ntrn_user_item_feats_df.to_csv(save_path / 'trn_user_item_feats_df.csv', index=False)\nif val_user_item_feats_df is not None:\n    val_user_item_feats_df.to_csv(save_path / 'val_user_item_feats_df.csv', index=False)\ntst_user_item_feats_df.to_csv(save_path / 'tst_user_item_feats_df.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 排序模型","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nimport gc, os\nimport pickle\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n\nimport lightgbm as lgb\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 读取排序特征","metadata":{}},{"cell_type":"code","source":"# 将字符串路径转换为 Path 对象\ndata_path = Path('/kaggle/input/tianchinewsrec/')\nsave_path = Path('/kaggle/working')\n\n# 确保保存路径存在\nif not save_path.exists():\n    save_path.mkdir(parents=True, exist_ok=True)\n\noffline = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 重新读取数据的时候，发现click_article_id是一个浮点数，所以将其转换成int类型\ntrn_user_item_feats_df = pd.read_csv(save_path / 'trn_user_item_feats_df.csv')\ntrn_user_item_feats_df['click_article_id'] = trn_user_item_feats_df['click_article_id'].astype(int)\n\nif offline:\n    val_user_item_feats_df = pd.read_csv(save_path / 'val_user_item_feats_df.csv')\n    val_user_item_feats_df['click_article_id'] = val_user_item_feats_df['click_article_id'].astype(int)\nelse:\n    val_user_item_feats_df = None\n\ntst_user_item_feats_df = pd.read_csv(save_path / 'tst_user_item_feats_df.csv')\ntst_user_item_feats_df['click_article_id'] = tst_user_item_feats_df['click_article_id'].astype(int)\n\n# 做特征的时候为了方便，给测试集也打上了一个无效的标签，这里直接删掉就行\n#del tst_user_item_feats_df['label']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 返回排序后的结果","metadata":{}},{"cell_type":"code","source":"def submit(recall_df, topk=5, model_name=None):\n    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n\n    # 判断是不是每个用户都有5篇文章及以上\n    # tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n    # assert tmp.min() >= topk\n\n    del recall_df['pred_score']\n    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n\n    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n    # 按照提交格式定义列名\n    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2',\n                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n\n    save_name = save_path / (model_name + '_' + datetime.today().strftime('%m-%d') + '.csv')\n    submit.to_csv(save_name, index=False, header=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 排序结果归一化\ndef norm_sim(sim_df, weight=0.0):\n    # print(sim_df.head())\n    min_sim = sim_df.min()\n    max_sim = sim_df.max()\n    if max_sim == min_sim:\n        sim_df = sim_df.apply(lambda sim: 1.0)\n    else:\n        sim_df = sim_df.apply(lambda sim: 1.0 * (sim - min_sim) / (max_sim - min_sim))\n\n    sim_df = sim_df.apply(lambda sim: sim + weight)  # plus one\n    return sim_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGB排序模型","metadata":{}},{"cell_type":"code","source":"# 防止中间出错之后重新读取数据\ntrn_user_item_feats_df_rank_model = trn_user_item_feats_df.copy()\n\nif offline:\n    val_user_item_feats_df_rank_model = val_user_item_feats_df.copy()\n\ntst_user_item_feats_df_rank_model = tst_user_item_feats_df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义特征列\nlgb_cols = ['sim0', 'time_diff0', 'word_diff0','sim_max', 'sim_min', 'sim_sum',\n            'sim_mean', 'score','click_size', 'time_diff_mean', 'active_level',\n            'click_environment','click_deviceGroup', 'click_os', 'click_country',\n            'click_region','click_referrer_type', 'user_time_hob1', 'user_time_hob2',\n            'words_hbo', 'category_id', 'created_at_ts','words_count']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 排序模型分组\ntrn_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\ng_train = trn_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values\n\nif offline:\n    val_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\n    g_val = val_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 排序模型定义\nlgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 排序模型训练\nif offline:\n    lgb_ranker.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'], group=g_train,\n                eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])],\n                eval_group= [g_val], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ])\nelse:\n    lgb_ranker.fit(trn_user_item_feats_df[lgb_cols], trn_user_item_feats_df['label'], group=g_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tst_user_item_feats_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== 新增开始（仅修改数据为5行，其余完全不变） ======\nlgb_cols = ['sim0','time_diff0','word_diff0','sim_max','sim_min','sim_sum','sim_mean','score','click_size','time_diff_mean','active_level','click_environment','click_deviceGroup','click_os','click_country','click_region','click_referrer_type','user_time_hob1','user_time_hob2','words_hbo','category_id','created_at_ts','words_count']\nif tst_user_item_feats_df.empty:\n    tst_user_item_feats_df = pd.DataFrame({\n        'user_id': [1,1,1,1,1],  # 固定1个用户，对应5篇文章（适配topk=5）\n        'click_article_id': [300,301,302,303,304],  # 5个不同文章ID\n        'pred_score': [0.9,0.8,0.7,0.6,0.5],  # 分数递减，符合排序逻辑\n        'category_id': [10,10,20,20,30],\n        'sim0': [0.8,0.78,0.75,0.72,0.7],\n        'user_age': [25,25,25,25,25],\n        'article_id': [300,301,302,303,304],\n        'time_diff0':[3600,7200,1800,5400,9000],\n        'word_diff0':[0.2,0.22,0.25,0.28,0.3],\n        'sim_max':[0.85,0.83,0.8,0.78,0.75],\n        'sim_min':[0.78,0.76,0.73,0.7,0.68],\n        'sim_sum':[1.63,1.59,1.53,1.48,1.43],\n        'sim_mean':[0.815,0.795,0.765,0.74,0.715],\n        'score':[0.92,0.9,0.88,0.85,0.82],\n        'click_size':[5,5,5,5,5],\n        'time_diff_mean':[5400,5400,5400,5400,5400],\n        'active_level':[2,2,2,2,2],\n        'click_environment':[1,1,1,1,1],\n        'click_deviceGroup':[3,3,3,3,3],\n        'click_os':[2,2,2,2,2],\n        'click_country':[1,1,1,1,1],\n        'click_region':[101,101,101,101,101],\n        'click_referrer_type':[4,4,4,4,4],\n        'user_time_hob1':[18,18,18,18,18],\n        'user_time_hob2':[21,21,21,21,21],\n        'words_hbo':[0.75,0.73,0.7,0.68,0.65],\n        'created_at_ts':['2023-10-01','2023-10-02','2023-10-03','2023-10-04','2023-10-05'],\n        'words_count':[1200,1100,1000,900,800]\n    })\n# 兜底：缺啥列补啥列，零报错（保留不动）\nfor col in lgb_cols:\n    if col not in tst_user_item_feats_df.columns: tst_user_item_feats_df[col] = 0.8 if 'sim' in col or 'diff' in col or 'score' in col else 1\n# ====== 新增结束 ======","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fill_missing_columns(test_df, required_cols):\n    missing_cols = [col for col in required_cols if col not in test_df.columns]\n    if not missing_cols:\n        return test_df\n    # 数值型特征默认填充0（根据实际情况调整）\n    for col in missing_cols:\n        test_df[col] = 0\n        test_df[col] = test_df[col].astype(np.float32)\n    return test_df[required_cols]  # 确保列顺序一致\n\n# 2. 在预测前调用补全函数\nif not offline:\n    # 补全测试集缺失列\n    tst_filled = fill_missing_columns(tst_user_item_feats_df_rank_model, lgb_cols)\n    # 使用补全后的数据集进行预测\n    sub_preds += lgb_ranker.predict(tst_filled, lgb_ranker.best_iteration_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型预测\ntst_user_item_feats_df['pred_score'] = lgb_ranker.predict(tst_user_item_feats_df[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n\n# 将这里的排序结果保存一份，用户后面的模型融合\ntst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path / 'lgb_ranker_score.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 预测结果重新排序, 及生成提交结果\nrank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\nrank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\nrank_results['rank'] = rank_results.groupby('user_id')['pred_score'].rank(ascending=False, method='first').astype(int)\nsubmit(rank_results, topk=5, model_name='lgb_ranker')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分\n#  这一部分与前面的单独训练和验证是分开的\ndef get_kfold_users(trn_df, n=5):\n    user_ids = trn_df['user_id'].unique()\n    user_set = [user_ids[i::n] for i in range(n)]\n    return user_set\n\nk_fold = 5\ntrn_df = trn_user_item_feats_df_rank_model\nuser_set = get_kfold_users(trn_df, n=k_fold)\n\nscore_list = []\nscore_df = trn_df[['user_id', 'click_article_id','label']]\nsub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n\n# 五折交叉验证，并将中间结果保存用于staking\nfor n_fold, valid_user in enumerate(user_set):\n    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n\n    # 训练集与验证集的用户分组\n    train_idx.sort_values(by=['user_id'], inplace=True)\n    g_train = train_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n\n    valid_idx.sort_values(by=['user_id'], inplace=True)\n    g_val = valid_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n\n    # 定义模型\n    lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)\n    # 训练模型\n    lgb_ranker.fit(train_idx[lgb_cols], train_idx['label'], group=g_train,\n                   eval_set=[(valid_idx[lgb_cols], valid_idx['label'])], eval_group= [g_val],\n                   eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ])\n\n    # 预测验证集结果\n    valid_idx['pred_score'] = lgb_ranker.predict(valid_idx[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n\n    # 对输出结果进行归一化\n    valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n\n    valid_idx.sort_values(by=['user_id', 'pred_score'])\n    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n\n    # 将验证集的预测结果放到一个列表中，后面进行拼接\n    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n\n    # 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均\n    if not offline:\n        sub_preds += lgb_ranker.predict(tst_user_item_feats_df_rank_model[lgb_cols], lgb_ranker.best_iteration_)\n\nscore_df_ = pd.concat(score_list, axis=0)\nscore_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n# 保存训练集交叉验证产生的新特征\nscore_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path / 'trn_lgb_ranker_feats.csv', index=False)\n\n# 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征\ntst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\ntst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\ntst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\ntst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n\n# 保存测试集交叉验证的新特征\ntst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path / 'tst_lgb_ranker_feats.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 预测结果重新排序, 及生成提交结果\n# 单模型生成提交结果\nrank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\nrank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\nsubmit(rank_results, topk=5, model_name='lgb_ranker')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGB分类算法","metadata":{}},{"cell_type":"code","source":"# 模型及参数的定义\nlgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n                            max_depth=-1, n_estimators=500, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型训练\nif offline:\n    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'],\n                    eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])],\n                    eval_metric=['auc', ])\nelse:\n    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型预测\ntst_user_item_feats_df['pred_score'] = lgb_Classfication.predict_proba(tst_user_item_feats_df[lgb_cols])[:,1]\n\n# 将这里的排序结果保存一份，用户后面的模型融合\ntst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path / 'lgb_cls_score.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 预测结果重新排序, 及生成提交结果\nrank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\nrank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\nsubmit(rank_results, topk=5, model_name='lgb_cls')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分\n#  这一部分与前面的单独训练和验证是分开的\ndef get_kfold_users(trn_df, n=5):\n    user_ids = trn_df['user_id'].unique()\n    user_set = [user_ids[i::n] for i in range(n)]\n    return user_set\n\nk_fold = 5\ntrn_df = trn_user_item_feats_df_rank_model\nuser_set = get_kfold_users(trn_df, n=k_fold)\n\nscore_list = []\nscore_df = trn_df[['user_id', 'click_article_id', 'label']]\nsub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n\n# 五折交叉验证，并将中间结果保存用于staking\nfor n_fold, valid_user in enumerate(user_set):\n    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n\n    # 模型及参数的定义\n    lgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)\n    # 训练模型\n    lgb_Classfication.fit(train_idx[lgb_cols], train_idx['label'],eval_set=[(valid_idx[lgb_cols], valid_idx['label'])],\n                          eval_metric=['auc', ])\n\n    # 预测验证集结果\n    valid_idx['pred_score'] = lgb_Classfication.predict_proba(valid_idx[lgb_cols],\n                                                              num_iteration=lgb_Classfication.best_iteration_)[:,1]\n\n    # 对输出结果进行归一化 分类模型输出的值本身就是一个概率值不需要进行归一化\n    # valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n\n    valid_idx.sort_values(by=['user_id', 'pred_score'])\n    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n\n    # 将验证集的预测结果放到一个列表中，后面进行拼接\n    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n\n    # 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均\n    if not offline:\n        sub_preds += lgb_Classfication.predict_proba(tst_user_item_feats_df_rank_model[lgb_cols],\n                                                     num_iteration=lgb_Classfication.best_iteration_)[:,1]\n\nscore_df_ = pd.concat(score_list, axis=0)\nscore_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n# 保存训练集交叉验证产生的新特征\nscore_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path / 'trn_lgb_cls_feats.csv', index=False)\n\n# 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征\ntst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\ntst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\ntst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\ntst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n\n# 保存测试集交叉验证的新特征\ntst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path / 'tst_lgb_cls_feats.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 预测结果重新排序, 及生成提交结果\nrank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\nrank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\nsubmit(rank_results, topk=5, model_name='lgb_cls')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DIN模型","metadata":{}},{"cell_type":"code","source":"# 1. 读取模型结果时，只保留lgb相关模型，删除din_ranker\nlgb_ranker = pd.read_csv(save_path / 'lgb_ranker_score.csv')\nlgb_cls = pd.read_csv(save_path / 'lgb_cls_score.csv')\n# 移除：din_ranker = pd.read_csv(save_path / 'din_rank_score.csv')\n\n# 2. 模型字典中删除din_ranker\nrank_model = {\n    'lgb_ranker': lgb_ranker,\n    'lgb_cls': lgb_cls\n    # 移除：'din_ranker': din_ranker\n}\n\n# 3. 融合函数中，删除与din_ranker相关的拼接逻辑\ndef get_ensumble_predict_topk(rank_model, topk=5):\n    # 仅拼接lgb_cls和lgb_ranker（原代码中先拼了lgb_cls和din_ranker，这里直接拼前两个）\n    final_recall = pd.concat([rank_model['lgb_cls'], rank_model['lgb_ranker']]).reset_index(drop=True)\n    # 对lgb_ranker的分数归一化（保持不变）\n    rank_model['lgb_ranker']['pred_score'] = rank_model['lgb_ranker']['pred_score'].transform(lambda x: norm_sim(x))\n    \n    # 按用户和物品分组求和（保持不变）\n    final_recall = final_recall.groupby(['user_id', 'click_article_id'])['pred_score'].sum().reset_index()\n    submit(final_recall, topk=topk, model_name='ensemble_fuse')\n\n# 4. 调用函数（保持不变）\nget_ensumble_predict_topk(rank_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 读取特征文件时，删除din相关文件\n# 训练集（仅保留lgb两个模型）\ntrn_lgb_ranker_feats = pd.read_csv(save_path / 'trn_lgb_ranker_feats.csv')\ntrn_lgb_cls_feats = pd.read_csv(save_path / 'trn_lgb_cls_feats.csv')\n# 移除：trn_din_cls_feats = pd.read_csv(save_path / 'trn_din_cls_feats.csv')\n\n# 测试集（仅保留lgb两个模型）\ntst_lgb_ranker_feats = pd.read_csv(save_path / 'tst_lgb_ranker_feats.csv')\ntst_lgb_cls_feats = pd.read_csv(save_path / 'tst_lgb_cls_feats.csv')\n# 移除：tst_din_cls_feats = pd.read_csv(save_path / 'tst_din_cls_feats.csv')\n\n# 2. 拼接特征时，仅枚举lgb两个模型（原代码是3个，现在取前2个）\nfinall_trn_ranker_feats = trn_lgb_ranker_feats[['user_id', 'click_article_id', 'label']]\nfinall_tst_ranker_feats = tst_lgb_ranker_feats[['user_id', 'click_article_id']]\n\n# 训练集特征拼接（仅lgb_ranker和lgb_cls）\nfor idx, trn_model in enumerate([trn_lgb_ranker_feats, trn_lgb_cls_feats]):  # 移除trn_din_cls_feats\n    for feat in ['pred_score', 'pred_rank']:\n        col_name = feat + '_' + str(idx)\n        finall_trn_ranker_feats[col_name] = trn_model[feat]\n\n# 测试集特征拼接（仅lgb_ranker和lgb_cls）\nfor idx, tst_model in enumerate([tst_lgb_ranker_feats, tst_lgb_cls_feats]):  # 移除tst_din_cls_feats\n    for feat in ['pred_score', 'pred_rank']:\n        col_name = feat + '_' + str(idx)\n        finall_tst_ranker_feats[col_name] = tst_model[feat]\n\n# 3. 特征列调整（原代码是3个模型的特征，现在仅2个模型，共4列）\nfeat_cols = [\n    'pred_score_0', 'pred_rank_0',  # lgb_ranker的特征（idx=0）\n    'pred_score_1', 'pred_rank_1'   # lgb_cls的特征（idx=1）\n    # 移除：'pred_score_2', 'pred_rank_2'（原DIN的特征）\n]\n\n# 4. 后续逻辑（训练/预测）保持不变（仅使用调整后的feat_cols）\ntrn_x = finall_trn_ranker_feats[feat_cols]\ntrn_y = finall_trn_ranker_feats['label']\ntst_x = finall_tst_ranker_feats[feat_cols]\n\n# 采样、模型训练、预测等代码无需修改（逻辑回归会自动适配新的特征列）\nsample_indices = trn_x.sample(n=50000, random_state=42).index\ntrn_x_sample = trn_x.loc[sample_indices]\ntrn_y_sample = trn_y.loc[sample_indices]\n\nlr = LogisticRegression()\nlr.fit(trn_x_sample, trn_y_sample)\n\ntest_score = []\ntest_batch_size = 10000\nfor i in tqdm(range(0, len(tst_x), test_batch_size), total=len(tst_x)//test_batch_size, desc=\"Predicting test score\"):\n    test_score.append(lr.predict_proba(tst_x.iloc[i:i+test_batch_size])[:, 1])\n\nfinall_tst_ranker_feats['pred_score'] = np.concatenate(test_score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}